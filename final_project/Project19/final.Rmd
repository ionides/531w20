
title: "Are you confident in the company, or are you greedy for the price?"
output: 
  html_document:
    toc: true
    number_sections: true
    theme: flatly
    code_folding: "hide"
---


```{r,include=FALSE,warning=FALSE, message=FALSE}
# Initialziing working directory to "./531FinalProject" and cache=TRUE
# knitr::opts_chunk$set(root.dir="./531FinalProject",cache=TRUE)

```

```{r,include=FALSE,warning=FALSE, message=FALSE}
# loading libraries
library(zoo)
library(gridExtra)
library(grid)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(pomp)
library(foreach)
library(doParallel)
library(doRNG)
registerDoParallel()
registerDoRNG(1111)


```

<br/>**Abstract**<br/>
If you don’t know the condition of the market, at least you should know the condition of the company. Under the dynamic market conditions, there are companies that win by brand names and grow persistently over the years despite repressive events such as market calamities, economic downturns, and war. They are known as businesses with durable competitive advantages. An example is Coca-cola, the most popular soft drink company, which recovers its share price from the 1987 crash to exceed its previous peaks within 5 years.There are also companies that only win by prices and often struggle to cope with economic plunges. They are known as  price competitive businesses. An example is AT&T, a network service provider, which still hasn't recovered from an economic recession 10 years ago. Just by watching the news or hearing from friends, you may be holding shares of some persistently-growing companies such e.g. Coca-Cola, but likewise, also holding shares from some of the non-growing companies, e.g. AT&T. Think again why this is. Are you holding the share because it was once sold at a low price? Or are you holding the share because you truly have confidence in the future performance of the company? While a company’s share price is largely constituted by the public’s reaction in news reports, its immunity and resistance towards displeasing economic events are immensely defined by its fundamental health. A company’s fundamental health may be evaluated from the sentiments of it’s quarterly (10-Q) or annual (10-K) reports. In such a sense, company reports may be characterized as its “health records”. Just because a company’s share price is low doesn’t mean there is anything wrong about the company,and vice versa, because the company’s share price is high doesn’t mean it guarantees good performances in the future. With that said, this paper will focus  on examining a company’s  fundamental health conditions through a case study with Coca-Cola and AT&T’s report sentiments.<br/>
  
<br/>**Introduciton**<br/>
Coca-Cola and AT&T are selected as companies to study because they epitomize a fundamentally healthy and a fundamentally weak business, respectively. This allows for an excellent benchmark when used for identifying investment opportunities and making investment decisions. Also, the two companies exist in the market long enough to amount substantial evidence of their fundamental performances. Before examining the fundamental healths of the selected companies, the assumption of the Efficient Market Hypothesis (EMH) should be relaxed because it states that prices reflect all information about the market, as this approach of analyzing the company's report sentiments lies outside the source of share prices. Exploratory Data Analysis (EDA) is performed to inspect the contribution of each sentiment type towards the companies’ market trend as well as to compare it to the direction of their shares prices albeit making inferences on them. The Susceptible-Infected-Recovered (SIR) Markov Chain model is implemented to monitor the flow of the sentiment types between compartments. Both the local and the global likelihoods of sentiment counts are searched and evaluated using the Iterative Filtering Algorithm (IF2). Also, profile likelihoods for model parameters are estimated to serve the purpose of model predictions in the future. This paper is not aimed to exhaust on high prediction , but rather discuss methods to evaluate the health conditions of public-listed companies.<br/>



<br/>**Exploratory Data Analysis**<br/>
*Data Collection and Description*<br/>
Full Quarterly Reports (10-Q) dating from March 2001 to September 2019 are collected from the U.S. Securities and Exchange Commission (SEC) for both Coca-Cola and AT&T. The SEC stands out as a reliable source for obtaining corporate information since these information are constructed by corporate experts to best reflect the condition of the companies. Quarterly (10-Q) reports are chosen here to be analyzed because it is available more frequently than the annual (10-K) reports, which allows our models to better capture the changes in the time series of sentiments for each of the companies. Each 10-Q report is parsed to obtain all its embedded words. These words are passed through the SentimentIntensityAnalyzer() function from the “vaderSentiment” package in Python to categorize each word into one of the three sentiment buckets: the positive sentiment buckey, neutral sentiment bucket, and negative sentiment bucket. Note that the “vaderSentiment” package may be biased in attributing likelihoods to word categories because it utilizes a private lexicon to analyze text. In this paper, we set this concern aside. For a company, word count in each sentiment bucket is recorded throughout the 57 quarterly terms, allowing for a total of 57*3=171 sentiment measures. In addition, monthly closing prices for Coca-Cola and AT&T over the  past 228 months (19 years) are collected via Yahoo Finance. Yahoo Finance is one of the few  platforms that supplies abundant financial data to the public for free.<br/>


*Data Pre-processing:*<br/>
To make the sample size more concrete, sentiment measures in quarterly terms are converted to monthly terms through spline interpolation, a polynomial basis technique widely used by economists to estimate gap periods within each time interval. The na_interpolation(option=”spline”) function from the “imputeTS” package in R can achieve this job. After the conversion, the word count for each month simply implies the number of words for a quarterly report if it was reported in that month. Consequently, this provides us with sentiment measures over 228 months, or  228*3=684 data points to work with. These sentiment measures are not standardized since they are expressed in the same units throughout time.<br/>



<br/>*Preliminary Data Vizualization:*<br/>
```{r,fig.height=7,fig.width=10,warning=FALSE, message=FALSE,echo=FALSE}
# loading data and manipulate it into the right format
## KO
koTable<-read.csv("./data/koTable.csv",stringsAsFactors=FALSE)[,-1];koTable$Time<-koTable$Time%>%as.yearmon(format="%m/%Y")%>%as.Date()
## T
tTable<-read.csv("./data/tTable.csv",stringsAsFactors=FALSE)[,-1];tTable$Time<-tTable$Time%>%as.yearmon(format="%m/%Y")%>%as.Date()

# prepare table for vizualization
## KO
koTableVisual<-data.frame(Time=koTable$Time,Count=c(koTable$neu_count,koTable$neg_count,koTable$pos_count),Sentiment=factor(c(rep("neutral",228),rep("negative",228),rep("positive",228)),levels=c("neutral","negative","positive")),Price=koTable$price)
## T
tTableVisual<-data.frame(Time=tTable$Time,Count=c(tTable$neu_count,tTable$neg_count,tTable$pos_count),Sentiment=factor(c(rep("neutral",228),rep("negative",228),rep("positive",228)),levels=c("neutral","negative","positive")),Price=tTable$price)
  
# Vizualization
## KO
koVisual<-ggplot(koTableVisual,aes(x=Time))+geom_area(aes(y=Count,fill=Sentiment))+scale_fill_manual(values=c("neutral"="grey","negative"="blueviolet","positive"="darkgoldenrod2"))+scale_y_continuous(name="Word Count",sec.axis=sec_axis(~.*0.001,name="Share price"))+geom_line(aes(y=Price/0.001,color="darkgreen"),size=0.8)+scale_color_identity(name="Market",breaks=c("darkgreen"),labels=c("Share price"),guide = "legend")+labs(x="Time",title = "Frequencies of Positive, Neutral, and Negative Words on Annual Reports for KO (2001/01/01-2019/12/01)")+scale_x_date(date_labels="%b/%y",date_breaks="1 year")+theme(axis.line.y.right=element_line(color = "darkgreen",size=0.8),axis.text.y.right=element_text(color="darkgreen"),axis.title.y.right=element_text(color="darkgreen"),axis.text.x = element_text(angle = 30))
## T
tVisual<-ggplot(tTableVisual,aes(x=Time))+geom_area(aes(y=Count,fill=Sentiment))+scale_fill_manual(values=c("neutral"="grey","negative"="blueviolet","positive"="darkgoldenrod2"))+scale_y_continuous(name="Word Count",sec.axis=sec_axis(~.*0.001,name="Share price"))+geom_line(aes(y=Price/0.001,color="darkgreen"),size=0.8)+scale_color_identity(name="Market",breaks=c("darkgreen"),labels=c("Share price"),guide = "legend")+labs(x="Time",title = "Frequencies of Positive, Neutral, and Negative Words on Annual Reports for T (2001/01/01-2019/12/01)")+scale_x_date(date_labels="%b/%y",date_breaks="1 year")+theme(axis.line.y.right=element_line(color = "darkgreen",size=0.8),axis.text.y.right=element_text(color="darkgreen"),axis.title.y.right=element_text(color="darkgreen"),axis.text.x = element_text(angle = 30))

# Combining two visuals 
grid.arrange(koVisual,tVisual,ncol=1,nrow=2,bottom =textGrob("Figure 1 (a) The orange, purple, and grey region: Cumulative annual report word counts for Coca-Cola and AT&T over a 19 year period \n                (b) Market share price of Coca-Cola and AT&T over a 19 year period", x = 0,y = 0.5,just ="left", gp = gpar(fontsize = 10,font=8)))

```
<br/>The total number of words from the 10-Q reports alongside with the word sentiment distributions and share prices over the 19-year period are displayed (Figure 1). The orange area represents the word count that indicates positive sentiment , the purple area represents the word count that indicates negative sentiment, and the grey area  represents the word count that indicates neutral sentiment. The total number of word count is traced out by the area graphs stacking on top of each other. The green line shows the trend of share prices throughout time. For Coca-Cola , share prices seem to increase with the total number of words in its 10-Q reports, and for AT&T, share prices seem to fluctuate in a stagnant manner with the total number of words in its 10-Q reports. Here, share prices measure the performances of the companies under the influence of public emotions,  as report sentiments  induce such measures under the companies’ fundamental health conditions .What stands out from the plots is the difference in the companies’ ability to recover from a recession event. For example, after the 2008 global financial crisis, Coca-Cola’s performance seems to yield immediate upward hikes in its share prices as AT&T shows no signs of improvements but a continuously sluggish trend in share prices. Why is one company more capable to recover from a droop than  another? Again, this is because the company is fundamentally healthier than the other. Lets model the “health records” of  the selected companies to examine the motions of their fundamental health conditions. For simplicity purpose, only the "positive sentiment" of Coca-Cola is studied. The same framework can be replicated for different sentiments in reports for other companies.<br/>


<br/>**Dynamic Modeling under the Partially Observed Markov Process (POMP) framework:**<br>
*Susceptible-Infected-Recovered (SIR) Markov Chain Model: *<br/>
Flow Diagram:<br/>
$S \rightarrow I\rightarrow R$<br/>
In the flow diagram above, each letter represents a compartmental state, for instance, S represents the population susceptible to a disease, I represents the population that is infected by their disease,  and R represents the population that is recovering from a disease. We transform the 10-Q report sentiment count data for each company such that they fit into an epidemiological or simply an SIR framework. After this , the sentiment counts and their movements may be represented by the flow chart above. We redefine the compartmental states as follows:<br/>
S: Words that are susceptible to change from one sentiment state to another<br/>
I: Words that that are infected by negative sentiments and are now in the negative sentiment state<br/>
R: Words that recovered from being in the negative sentiment state and is now in the positive sentiment state<br/>
<br/>
Ordinary Differnetial Equation Interpretation (ODE) of the SIR Model- A discrete-time case:<br/> 
$\frac{dN_{SI}}{dt}=\mu_{SI}(t)S(t)$<br/>
$\frac{dN_{IR}}{dt}=\mu_{IR}I(t)$<br/>
Here, $dN_{.}$ denotes the number of population that leaves one compartment and enters into another.<br/> $\mu_{.}$ denotes the rate that population transit between compartments.<br/>
<br/>
The Counting Process-counts on words with three states of sentiment:<br/>
$S(t+1)=S(t)-dN_{SI}$<br/>
$I(t+1)=I(t)+dN_{SI}-dN_{IR}$<br/>
$R(t+1)=R(t)+dN_{IR}$<br/>
<br/>
Euler's Method- A binomial approximation with exponential transition probabilities: used to solve ODE's by approximating their respective transition quantities using binomial approximation. For example, <br/>
$dN_{SI}(t)\sim Binomial(S(t),1-exp(-\mu_{SI}(I(t))dt))$<br/>

<br/>
SIR Model Setup:<br>
```{r,warning=FALSE, message=FALSE,echo=FALSE}
# retrieving transformed data
koTableTrans<-read.csv("./data/koTableTrans.csv");koTableTrans<-koTableTrans[,-1]
tTableTrans<-read.csv("./data/tTableTrans.csv");tTableTrans<-tTableTrans[,-1]
#--------------------------------------------------------

############################# declaring Csnippets
ko_step <- Csnippet("
  double dN_SI = rbinom(S,1-exp(-Beta*I/N*dt));
  double dN_IR = rbinom(I,1-exp(-mu_IR*dt));
  S -= dN_SI;
  I += dN_SI - dN_IR;
  H += dN_IR;
")

ko_init <- Csnippet("
  S = nearbyint(eta*N);
  I = 1;
  H = 0;
")

ko_dmeas <- Csnippet("
  lik = dbinom(POS,H,rho,give_log);
")

ko_rmeas <- Csnippet("
  POS = rbinom(H,rho);
")

subset(koTableTrans,select=c(month,POS)) %>%
  pomp(
    times="month",t0=0,
    rprocess=euler(ko_step,delta.t=1/12),
    rinit=ko_init,
    rmeasure=ko_rmeas,
    dmeasure=ko_dmeas,
    accumvars="H",
    partrans=parameter_trans(log=c("Beta","mu_IR"),logit=c("rho","eta")),
    statenames=c("S","I","H"),
    paramnames=c("Beta","mu_IR","eta","rho","N"),
    cdir=".",cfile="tmp"
  ) -> koSIR

########################## setting parameters
params <- c(Beta=200,mu_IR=2,rho=0.9,eta=0.1,N=9000)
fixed_params <- c(N=9000, mu_IR=2)

####################### running s particle filter
registerDoParallel()
registerDoRNG(11551678)


foreach(i=1:10,.combine=c) %dopar% {
  library(pomp)
  koSIR %>% pfilter(params=params,Np=10000)
} -> pf

pf %>% logLik() %>% logmeanexp(se=TRUE) -> L_pf
L_pf

```
<br/>A paticle filter is run with 10000 particles for 10 iterations. The best result yields a likelihood of -8925.16 with a standard error of 0.2. We expand our likelihood exploration to the local and then the global regions.<br/>

<br/>A Local Search of the Likelihood Surface:<br/>
```{r,warning=FALSE, message=FALSE,echo=FALSE}
# Building a picture of the likelihood
pf[[1]] %>% coef() %>% bind_rows() %>%
  bind_cols(loglik=L_pf[1],loglik.se=L_pf[2]) %>%
  write_csv("koPOS_params.csv")

# carrying out local search
registerDoRNG(482947940)
bake(file="koPOS_local_search.rds",{
  foreach(i=1:20,.combine=c) %dopar%  
    {
      library(pomp)
      library(tidyverse)
      
      koSIR %>%
        mif2(
          params=params,
          Np=2000,
          Nmif=50,
          cooling.fraction.50=0.5,
          rw.sd=rw.sd(Beta=0.02,rho=0.02,eta=ivp(0.02))
        )

    }
}) -> koPOS_mifs_local

koPOS_local_search_plt<-koPOS_mifs_local %>%traces() %>%melt() %>%ggplot(aes(x=iteration,y=value,group=L1,color=factor(L1)))+geom_line()+guides(color=FALSE)+facet_wrap(~variable,scales="free_y")+ theme_bw()
koPOS_local_search_plt
```

<br/>Parameters are initialized at $\beta=200,\mu_{IR}=2,\rho=0.9,\eta=0.1,N=9000$. 2000 particles are used. A perturbation size of 0.02 is set for all estimated parameters on a log scale. The cooling fraction is set to 0.5 such that perturbations are reduced by half after 50 iterations. For simplicity, 50 iterations are executed. No filtering failures are generated after about 5 iterations, which is a good sign of filtering because the particle filters successfully decrease the random walk variance at each iteration. The local log-likelihood seems to converge, but however may not make desirable inferences because parameter perturbations exist for the last filtering iteration, which may lead to erroneous likelihood estimations for the parameters. The predictor estimations seem decent because as signs of convergence are shown, there are yet a few replications that deviate from the convergence region.<br/>


<br/>Evaluating the Local Likelihood Search:<br/>

```{r,warning=FALSE, message=FALSE,echo=FALSE}
registerDoRNG(900242057)
# evaluation
bake(file="koPOS_lik_local.rds",{

  foreach(mf=koPOS_mifs_local,.combine=rbind) %dopar% 
    {
      library(pomp)
      library(tidyverse)
      evals <- replicate(10, logLik(pfilter(mf,Np=20000)))
      ll <- logmeanexp(evals,se=TRUE)
      mf %>% coef() %>% bind_rows() %>%
        bind_cols(loglik=ll[1],loglik.se=ll[2])
    }
  
}) -> koPOS_local_results

# Investigate correlation plots
pairs(~loglik+Beta+eta+rho,data=koPOS_local_results,pch=16)

# saving local likelihoods into files
read_csv("koPOS_params.csv") %>%
  bind_rows(koPOS_local_results) %>%
  arrange(-loglik) %>%
  write_csv("koPOS_params.csv")
```
<br/>Parameter and the log-likelihood approximations are plotted in pairs to display the local likelihood surface. There does not seem to be any correlations between the estimated parameters. Other than this, few can be said about the paired plots since they are too dispersed to draw a clear picture of the likelihood surface.<br/>



<br/>A Global Search of the Likelihood Surface:<br/>
```{r,warning=FALSE, message=FALSE,echo=FALSE}
# carrying out search

registerDoRNG(1270401374)

runifDesign(
  lower=c(Beta=100,rho=0.2,eta=0),
  upper=c(Beta=300,rho=0.9,eta=0.4),
  nseq=10
) -> koPOS_guesses

koPOS_mf1 <- koPOS_mifs_local[[1]]

bake(file="koPOS_global_search10.rds",{
  foreach(guess=iter(koPOS_guesses,"row"), 
          .combine=rbind,
          .export=c("koPOS_mf1","fixed_params")
          ) %dopar% 
    {
      library(pomp)
      library(tidyverse)
      koPOS_mf1 %>%
        mif2(params=c(unlist(guess),fixed_params)) %>%
        mif2(Nmif=50) -> mf
      ll <- replicate(10,mf %>% pfilter(Np=2000) %>% logLik())
      ll <- logmeanexp(ll,se=TRUE)
      mf %>% coef() %>% bind_rows() %>%
        bind_cols(loglik=ll[1],loglik.se=ll[2])
    }
}) -> koPOS_global_results

# saving global likelihoods into file
read_csv("koPOS_params.csv") %>%
  bind_rows(koPOS_global_results) %>%
  arrange(-loglik) %>%
  write_csv("koPOS_params.csv")


```
<br/>A matrix containing self-specified parameter ranges is generated to grant starting values for the global likelihood search . For good parameter estimations, wider ranges of starting values should be specified. As this paper is aimed to introduce simulation methods under the dynamic modeling framework and  is not exhaustive in testing for model accuracy, a smaller matrix containing 10 starting values is used. Also, for simplicity, the same number of iterations, replications, and particle filters from the local likelihood searches are used in the global search. Because of the above reasons, it is expected that the predictability of the SIR POMP model is limited. We can see this from the paired plots below.<br/>

<br/>Evaluating the Global Likelihood Search:<br/>
```{r,warning=FALSE, message=FALSE,echo=FALSE}
# bind updated likelihood estimates to existing estimates
read_csv("koPOS_params.csv") %>%
  filter(loglik>max(loglik)-50) %>%
  bind_rows(koPOS_guesses) %>%
  mutate(type=if_else(is.na(loglik),"guess","result")) %>%
  arrange(type) -> all

# paired plots between all parameters 
pairs(~loglik+Beta+eta+rho, data=all,
      col=ifelse(all$type=="guess",grey(0.5),"red"),pch=16)
```
<br/>From the paired scatter plots above,  grey points representing the parameter starting values and red points indicate the parameter estimates from the IF2 algorithm. This is a  relatively  small sample range, yet compelling enough to see how the particle filters trace out the estimated likelihoods of each parameter.<br/> 

<br/>Is this likelihood maximization viable?<br/>
```{r,warning=FALSE, message=FALSE,echo=FALSE}
# paired plot between loglik and eta
all %>%
  filter(type=="result") %>%
  filter(loglik>max(loglik)-10) %>%
  ggplot(aes(x=eta,y=loglik))+
  geom_point()

```
<br/>In particular, the paired pot between $\eta$ and its log-likelihoods is displayed to demonstrate the convergence of the parameter estimations. This implication roots from the particles exploring the ranges of parameters and eventually found its way into the high likelihood regions. Therefore, this likelihood maximization seems viable.<br/>


<br/>Profile Likelihood of $\eta:$ <br/>
```{r,warning=FALSE, message=FALSE,echo=FALSE, include=FALSE}

# # extracting ranges of estimated parameters
# registerDoRNG(830007657)
# 
# read_csv("koPOS_params.csv") %>%
#   filter(loglik>max(loglik)-20,loglik.se<2) %>%
#   sapply(range) -> box
# 
# # designing matrix to contain point ranges for eta
# profileDesign(
#   eta=seq(0.01,0.85,length=40),
#   lower=box[1,c("Beta","rho")],
#   upper=box[2,c("Beta","rho")],
#   nprof=15, type="runif"
# ) -> guesses
# 
# # independent sequence of iterated filtering operations from each of these points
# 
# mf1 <- koPOS_mifs_local[[1]]
# 
# bake(file="koPOS_eta_profile.rds",{
# 
#   foreach(guess=iter(guesses,"row"),
#           .combine=rbind,
#           .export=c("mf1","fixed_params")
#           ) %dopar%
#     {
#       library(pomp)
#       library(tidyverse)
#       mf1 %>%
#         mif2(params=c(unlist(guess),fixed_params),
#              rw.sd=rw.sd(Beta=0.02,rho=0.02)) %>%
#         mif2(Nmif=50,cooling.fraction.50=0.3) -> mf
#       ll <- replicate(10,mf %>% pfilter(Np=2000) %>% logLik())
#       ll <- logmeanexp(ll,se=TRUE)
#       mf %>% coef() %>% bind_rows() %>%
#         bind_cols(loglik=ll[1],loglik.se=ll[2])
#     }
# 
# }) -> koPOS_results
# # save results in global database
# read_csv("koPOS_params.csv") %>%
# bind_rows(koPOS_results) %>%
# arrange(-loglik) %>%
# write_csv("koPOS_params.csv")
# 
# # plotting results
# read_csv("koPOS_params.csv") %>%
#   filter(loglik>max(loglik)-10) -> all
# 
# pairs(~loglik+Beta+eta+rho, data=all,pch=16)
# 
# # plot results for eta vs loglik
# koPOS_results %>%
#   group_by(round(eta,5)) %>%
#   filter(rank(-loglik)<3) %>%
#   ungroup() %>%
#   ggplot(aes(x=eta,y=loglik))+
#   geom_point()+
#   geom_smooth(method="loess",span=0.25)+
#   geom_hline(color="red",yintercept=max(koPOS_results$loglik)-0.5*qchisq(df=1,p=0.95))+lims(y=max(koPOS_results$loglik)-c(5,0))


```

![](./figures/eta_proflik.png)

<br/> In the plot above, the blue line traces out the likelihood of $\eta$ as the red bar fixes a cutoff at a 95% confidence interval (CI). The 95% confidence interval does not cover the likelihood function. This means that the current state of the simulated $\eta's$ are not significant for the model at a 95% CI. In other words, with either more simulations or tunings for parameters such as the number of particles, cooling fractions, and  perturbations, the iterative algorathm may avoid estimations from the low confidence region, and overcome the noises as it learns from the specified ranges.<br/>

<br/>*Ricker Model- Another feasible model for processing sentiemnt counts:*<br/>
$P_{n+1}=rP_{n}exp(-\frac{P_{n}}{k})$<br/>
$Y_{n}\mid P_{n}\sim Negbin(\phi P_{n},\psi)$<br/>
Here,$r$ denotes the intrinsic growth rate of the 10-Q report word count and $k$ denotes the word limit of the report.<br/>

<br/>Another perspective can be taken to interpret the 10-Q report word counts. Like eoclogical populations, total word counts from one year may be carried over to the next year because the same group of corporate professionals who wrote the report year after year such that its wordings and sentiments may be simliar. The Ricker model is often appropriate in modeling population growth, but in this case, may also be used to model sentiment counts of company reports. Under such framework, no transformation will be needed, thus aiding fewer information loss. However, note that reparameterization may be required so that the scaling of the $P_{n}$ is explicit. Take Coca-Cola for example, we may assume that total word counts for its quarterly reports succeeds those of the previous years. The Ricker model added an exponential growth component to the reports' total word counts, making it a proper model to fit the growing contents of Coca-Cola's quarterly report. One may even break down the model by sentiments to scrutinize the sentimental make-up of the report. Or to put this another way, one may dissect a company's health record and examine its history of transitioning between a "healthy state" (postive sentiment) and an "unhealthy state" (negative sentiment).

<br/>*Other Plausible Models:*<br/>
(1) Malthusian Growth Model: (https://en.wikipedia.org/wiki/Malthusian_growth_model)<br/>
(2) Fibonacci Population Growth Model: (https://math.temple.edu/~reich/Fib/fibo.html)<br/>
(3) Verhulst-Pearl (logistic) Model: (https://en.wikipedia.org/wiki/Pierre_Fran%C3%A7ois_Verhulst)<br/>




<br/>**Conclusion and Thoughts**<br/>
To conclude, dynamic modeling under the POMP framework thorougly moniters the movements of the company's sentiments. In particular, the SIR model captures an important concern in hte business world: "How good is the company feeling about its performance?". The IF2 goes beyond answering this question and estimates what the company will feel about their performances in the future. It accomplished this by exploring local regions of the incoming and leaving of word counts that indicates "positive sentiment", and later explores the global regions of these events. High estimates of words leaving the positive state implies a decline in the health of the company while high estimates of words entering the positive state implies an improvement in health of the company. Only the "health records" of Coca-Cola is studied in this paper. With more time, other veteran companies such as Hersey’s, Moody’s, and Taco Bell, may also be accounted for a more holistic implication. Note that filtering parameters are not changed around too much due to computational limits; more robust estimates may be obtained if particle filters and iterations are set to a higher notch, or if perturbations or cooling factors are changed to yield faster convergence. Coca-Cola is a good starting point, but there are yet many other companies with different sentiemnt properties such that this approach may not stand. The primary takaway is that other than share prices, it is possible to compare companies the dynamic modeling of sentiments from their quarterly (10-Q) or annual (10-K) reports.<br/>

**References**<br/>
[1] M.Ebrahimi,A.Yazdavar,A.Sheth, "OntheChallengesofSentimentAnalysisforDynamicEvents",IEEEIntelligentSystems,2017 <br/>
[2] Mehran Azimi and Anup Agrawal,"Is Positive Sentiment in Corporate Annual Reports Informative? Evidence from Deep Learning",July 2019<br/>
[3] Foteini Kollintza-Kyriakoulia, Manolis Maragoudakis, and Anastasia Krithara,"Measuring the Impact of Financial News and Social
Media on Stock Market Modeling Using Time Series
Mining Techniques", 6 November 2018<br/>
[4] EDWARD L. IONIDES, ANINDYA BHADRA, YVES ATCHADÉ, and AARON KING,"ITERATED FILTERING",The Annals of Statistics 2011, Vol. 39, No. 3, 1776–1802 <br/>
[5] https://kingaa.github.io/sbied/mif/mif.html <br/>











