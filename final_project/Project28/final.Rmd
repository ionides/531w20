---
title: "Modelling the Volatilty of VI"
date: "April 28, 2020"
output: 
  html_document:
    theme: flatly
    toc: yes
---

```{r echo=FALSE}
library(knitr)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
opts_chunk$set(echo=FALSE)
```


# Introduction

The volatility of financial markets is a widely studied area. Understanding a market's volaility gives investors insight into how risky their investments may be. The Chicago Board Option Exchange (CBOE) publishes the Volatility Index (VIX), a financial benchmark that provides an estimate of the stock market's volatilty[1]. It is calculated based on the prices of S&P 500 index options. The CBOE also offers futures and options for VIX itself. For example, an investor can buy VIX calls if they belive the market will soon become more volatile than it currently is. These VIX instruments provide investors with the opportunity to manage the risk of their other investments. 

In this project, I will apply existing volatility models for stock prices on the value of the VIX benchmark. We will see whether the volatility of stock prices act in a similar manner to the volatility of the volatilty of the market. I will experiement with two classes of models: partially observed Markov processes (POMP) and GARCH.

# Data

The data is of the VIX index from the beginning of 2018 to the end of 2019 downloaded from the CBOE website[1]. While there was data available up to the current day (April 2020), the dramatic increase in market volatilty due to the COVID-19 pandemic may be difficult to model due to its anomalous nature. This leaves us with a little over 500 days of data.

```{r}
library(ggplot2)
theme_set(theme_bw())
library(tidyverse)
library(plyr)
library(reshape2)
library(foreach)
library(pomp)
```

```{r}
vix_data = read.csv('vixcurrent.csv')
vix_data$Date = as.Date(vix_data$Date, format = "%m/%d/%y")
vix_data = vix_data[(vix_data$Date >= "2018-01-01") & (vix_data$Date <= "2019-12-31"), ]
plot(x = vix_data$Date, y = vix_data$VIX.Close, type = 'l', main = 'VIX', ylab = 'Index Value', xlab='Date')
```

We can see that the index has an average value between 15 and 20 with very sharp peaks at the beginning and ending of 2018. We will now look at demeaned log returns of the index as they are more likely to form a stationary process without trend that will be easier to analyze.

```{r}
logvix = diff(log(vix_data$VIX.Close))
logvix.demean = logvix - mean(logvix)
plot(x = vix_data$Date[2:503], y =logvix.demean, type = 'l', main='Demeaned VIX Log Returns', xlab='Date', ylab='Log Return')
```

Here we can see that there is no easily indentifiable trend. Outside of the sharp peak at the beginning of 2018, there is no clear trend in the volatilty of the returns either. This will be the time series the rest of our analysis will be performed on.

# POMP Model

## Leverage Model

Leverage refers to the phenomenon that large losses dealt to a stock market index are followed by increases in market volatility [2]. Leverage, $R_n$, on day $n$ can be formally quantified as the correlation between the index return on day $n-1$ and increase in the log volatilty from day $n-1$ to $n$. We will be using a model introduced by Bretó which models $R_n$ as: 
$$R_n = \frac{\exp\{2G_n\}-1}{\exp\{2G_n\}+1} $$ where $G_n$ is a Gaussian random walk [3]. 

## Stochastic Volatility Model

Using Bretó's model, we can model volatilty as: 
$$Y_n = \exp(\frac{H_n}{2})\epsilon_n \\ H_n = \mu_h(1-\phi)+\phi H_{n-1} + \beta_{n-1}R_n\exp(\frac{-H_{n-1}}{2})+W_n \\ G_n = G_{n-1} + V_n$$

where: 
$$H_n = \log(\sigma^2_n) = 2\log(\sigma_n) \\ \beta_n = Y_n\sigma_\eta\sqrt{1-\phi^2} \\ \epsilon_n \sim \text{i.i.d } \mathcal{N}(0,1) \\ W_n \sim \text{i.i.d } \mathcal{N}(0,\sigma^2_\omega) \\ V_n \sim \text{i.i.d } \mathcal{N}(0,\sigma^2_v) \\ \sigma_\omega = \sigma_\eta\sqrt{(1-\phi^2)(1-R^2_n)}$$
In the context of our POMP model, $G_n$ is unobservable; $Y_n$, the return, is observable; and $H_n$, the log volatilty, is unobservable.

## 1Computing the POMP model

We can now estimate the parameters for the POMP model.

```{r}
require(pomp)
vix_statenames = c("H","G","Y_state")
vix_rpnames = c("sigma_nu","mu_h","phi","sigma_eta")
vix_ivpnames = c("G_0","H_0")
vix_paramnames = c(vix_rpnames, vix_ivpnames)
covarnames = "covaryt"

rproc1 <- "
  double beta,omega,nu;
  omega = rnorm(0,sigma_eta * sqrt( 1- phi*phi ) * sqrt(1-tanh(G)*tanh(G)));
  nu = rnorm(0, sigma_nu);
  G += nu;
  beta = Y_state * sigma_eta * sqrt( 1- phi*phi );
  H =   mu_h*(1 - phi) +phi*H + beta * tanh(G) * exp(-H/2) + omega;
"
rproc2.sim <- "
  Y_state = rnorm( 0,exp(H/2) );
 "

rproc2.filt <- "
  Y_state = covaryt;
 "

vix_rproc.sim = paste(rproc1, rproc2.sim)
vix_rproc.filt = paste(rproc1, rproc2.filt)
```

```{r}
vix_rinit <- "
  G = G_0;
  H = H_0;
  Y_state = rnorm( 0,exp(H/2) );
"

vix_rmeasure <- "
   y=Y_state;
"

vix_dmeasure <- "
   lik=dnorm(y,0,exp(H/2),give_log);
"

vix_partrans <- parameter_trans(
  log=c("sigma_eta","sigma_nu"),
  logit="phi"
)


vix_covar <- covariate_table(
  time=0:length(logvix.demean),
  covaryt = c(0, logvix.demean),
  times="time"
)

```

```{r}
vix.filt <- pomp(data=data.frame(y=logvix.demean,
                     time=1:length(logvix.demean)),
              statenames=vix_statenames,
              paramnames=vix_paramnames,
              times="time",
              t0=0,
              covar=vix_covar,
              rmeasure=Csnippet(vix_rmeasure),
              dmeasure=Csnippet(vix_dmeasure),
              rprocess=discrete_time(step.fun=Csnippet(vix_rproc.filt),delta.t=1),
              rinit = Csnippet(vix_rinit),
              partrans = vix_partrans
)
```

```{r}
expit<-function(real){1/(1+exp(-real))}
logit<-function(p.arg){log(p.arg/(1-p.arg))}
params_test <- c(
                  sigma_nu = exp(-4.5),
                  mu_h = -0.25,
                  phi = expit(4),
                  sigma_eta = exp(-0.07),
                  G_0 = 0,
                  H_0=0
)
sim1.sim = pomp(vix.filt,
                statenames=vix_statenames,
                paramnames=vix_paramnames,
                rprocess=discrete_time(
                step.fun=Csnippet(vix_rproc.sim),delta.t=1)
)

sim1.sim = simulate(sim1.sim,seed=1,params=params_test)
```

```{r}
sim1.filt <- pomp(sim1.sim,
                  covar=covariate_table(
                  time=c(timezero(sim1.sim),time(sim1.sim)),
                  covaryt=c(obs(sim1.sim),NA),
                  times="time"),
                  statenames=vix_statenames,
                  paramnames=vix_paramnames,
                  rprocess=discrete_time(step.fun=Csnippet(vix_rproc.filt),delta.t=1)
)
```

```{r}
run_level <- 1
vix_Np <- switch(run_level,100,1e3,2e3)
vix_Nmif <- switch(run_level,10, 100,200)
vix_Nreps_eval <- switch(run_level,4, 10, 20)
vix_Nreps_local <- switch(run_level,10, 20, 20)
vix_Nreps_global <- switch(run_level,10, 20, 100)
```

```{r}
require(doParallel)
registerDoParallel()
```

```{r}
vix_rw.sd_rp <- 0.02
vix_rw.sd_ivp <- 0.1
vix_cooling.fraction.50 <- 0.5

stew("mif1.rda",{
   t.if1 <- system.time({
   if1 <- foreach(i=1:vix_Nreps_local,
     .packages='pomp', .combine=c) %dopar% mif2(vix.filt,
          params=params_test,
          Np=vix_Np,
          Nmif=vix_Nmif,
          cooling.fraction.50=vix_cooling.fraction.50,
          rw.sd = rw.sd(
                            sigma_nu  = vix_rw.sd_rp,
                            mu_h      = vix_rw.sd_rp,
                            phi       = vix_rw.sd_rp,
                            sigma_eta = vix_rw.sd_rp,
                            G_0       = ivp(vix_rw.sd_ivp),
                            H_0       = ivp(vix_rw.sd_ivp)
          )
       )
   L.if1 <- foreach(i=1:vix_Nreps_local,
     .packages='pomp', .combine=rbind) %dopar% logmeanexp(
       replicate(vix_Nreps_eval,
         logLik(pfilter(vix.filt,params=coef(if1[[i]]),Np=vix_Np))
       ), se=TRUE)
  })
},seed=318817883,kind="L'Ecuyer")
```

```{r}
r.if1 <- data.frame(logLik=L.if1[,1],logLik_se=L.if1[,2],t(sapply(if1,coef)))
if (run_level>1) 
  write.table(r.if1,file="vix_params.csv",append=TRUE,col.names=FALSE,row.names=FALSE)
summary(r.if1$logLik,digits=5)
```

We can see that our iterated filtering processes end us with a median log likelihood of 571.4. We can see the diagnostics of the model.

```{r}
pairs(~logLik+sigma_nu+mu_h+phi+sigma_eta,data=subset(r.if1,logLik>max(logLik)-20))
plot(if1)
```

From the pairwise correlation graph, we can see that the likelihood is maximized over a range of most parameters. From the convergence diagnostics, about half of the particles achieved a log likelihood of about 570 while the other half only achieved about 540. While some of the parameters seem to have converged, others have not. Instead of randomly initializing parameters we can instead define a range from which they should be selected.

```{r}
vix_box <- rbind(
 sigma_nu=c(0.005,0.05),
 mu_h    =c(-1,0),
 phi = c(0.95,0.99),
 sigma_eta = c(0.5,1),
 G_0 = c(-2,2),
 H_0 = c(-1,1)
)
```

```{r}
stew(file="box_eval.rda",{
  t.box <- system.time({
    if.box <- foreach(i=1:vix_Nreps_global,
      .packages='pomp',.combine=c) %dopar%  
      mif2(
        if1[[1]],
        params=apply(vix_box,1,function(x)runif(1,x))
      )
    
    L.box <- foreach(i=1:vix_Nreps_global,
      .packages='pomp',.combine=rbind) %dopar% {
                        set.seed(87932+i)
                        logmeanexp(
                          replicate(vix_Nreps_eval,
                                    logLik(pfilter(vix.filt,params=coef(if.box[[i]]),Np=vix_Np))
                          ), 
                          se=TRUE)
                      }
  })
},seed=290860873,kind="L'Ecuyer")
r.box <- data.frame(logLik=L.box[,1],logLik_se=L.box[,2],t(sapply(if1,coef)))
if (run_level>1) 
  write.table(r.if1,file="vix_params.csv",append=TRUE,col.names=FALSE,row.names=FALSE)
summary(r.box$logLik,digits=5)
```

These likelihoods are slightly greater than before. Again, we can look at the diagnostics. 

```{r}
pairs(~logLik+sigma_nu+mu_h+phi+sigma_eta,data=subset(r.box,logLik>max(logLik)-10))
r.box <- data.frame(logLik=L.box[,1],logLik_se=L.box[,2],t(sapply(if.box,coef)))
if(run_level>1) write.table(r.box,file="vix_params.csv",append=TRUE,col.names=FALSE,row.names=FALSE)

plot(if.box)
```

We can see that all particles converged to about the same likelihood unlike last time. The parameters also converge better, but $\mu_h$ and $\sigma_\eta$ still do not converge well. While this model is better than the previous model, we cannot be very certain that it is a good model. 

# GARCH Models

Generalized autoregressive heteroscedasticity (GARCH) models are widely used to model financial time series. A process $X_n$ is GARCH(p,q) if: $$X_n = \mu_n + \sigma_n\epsilon_n$$ where $\epsilon_n$ is a white noise process with mean 0 and variance 1, and: $$\sigma^2_n = \alpha_0 + \sum_{i=1}^p\beta_i\sigma_{n-i}^2+\sum_{j=1}^q\alpha_j\tilde{X}_{n-j}^2$$ where $\tilde{X}_n = X_n - \mu_n$ [4]. GARCH processes are self exciting in that current volatility simulates an increase in volatility in the near future. GARCH models are good when the error in the volatilties follow an ARMA process. In practice, GARCH(1,1) is used, and it is rare to use a GARCH model of higher order. We will fit a GARCH(1,1) model to our time series of VIX returns.


```{r}
library(fGarch)
fit.garch = garchFit(~garch(1,1), logvix.demean, trace=F)
summary(fit.garch)
```

We can see that the log likelihood is 547.4, which is lower than the POMP model. However, by default, the model assumes the residuals follow a Gaussian normal distribution. From the summary, we can see that the Shaprio-Wilk Test gave a p-value very close to 0, indicating that the residuals are not normally distributed. A look at the QQ plots verifies this.

```{r}
qqnorm(fit.garch@residuals/fit.garch@sigma.t)
qqline(fit.garch@residuals/fit.garch@sigma.t)
```

We can see that the right end of the data is heavier tailed and the left end is lighter tailed than a normal distribution. Therefore, we can instead fit a model where the residuals follow a skewed Student's t distribution. We also will explore combining the GARCH model with an ARMA model. Here, the conditional mean of the index is modeled using an ARMA process, and its conditional variance is modeled with a GARCH process [5]. We will fit ARMA(p,q)-GARCH(1,1) models for p and q from 0 to 5 and compare the AIC values of each model.

```{r}
aic_table <- function(data,P,Q){
  table <- matrix(NA,(P+1),(Q+1))
  for(p in 0:P) {
    for(q in 0:Q) {
      tempfit = garchFit(substitute(~arma(p,q)+garch(1,1),list(p=p,q=q)), data = data, grad = "numerical", trace = F, cond.dist = 'sstd')
      table[p+1,q+1] <- tempfit@fit$ics[1]
    }
  }
  dimnames(table) <- list(paste("<b> p",0:P, "</b>", sep=""),paste("q",0:Q,sep=""))
  table
}
vix_aic_table <- aic_table(logvix.demean,5,5)
```

```{r}
require(knitr)
kable(vix_aic_table, digits = 5)
```

From the table, we can see that the ARMA(3,3)-GARCH(1,1) model has the lowest AIC value. However, this model was found to have invertibility issues. Other models with low AIC values could not have their parameters estimated with significance. The best viable model turned out to be the ARMA(1,1)-GARCH(1,1) model. We fit this model to the data.

```{r}
vix.garch.fit = garchFit(~arma(1,1)+garch(1,1), data = logvix.demean,grad = "numerical", trace = F, cond.dist = 'sstd')
summary(vix.garch.fit)
```

We can see that all the parameters are significant, and that the log likelihood is 603.9, greater than our best POMP model. We can see how well the residuals are modeled with the skewed Student's t distribution with 5.34 degrees of freedom and a skew parameter of 1.64.


```{r}
N = length(vix.garch.fit@residuals)
quantv = (1/N)*seq(.5,N-.5,1)
resids = vix.garch.fit@residuals/vix.garch.fit@sigma.t
skew = vix.garch.fit@fit$matcoef[7]
df =  vix.garch.fit@fit$matcoef[8]
qqplot(qsstd(quantv,nu=df, xi=skew), resids, main='QQ plot for t-dist on residuals', xlab = 'Quantile', ylab = 'Residual')
qqline(vix.garch.fit@residuals/vix.garch.fit@sigma.t, distribution = function(p) qsstd(p,nu=df, xi=skew), prob = c(0.1,0.9), col=2)
```

We can see that this model better captures the residuals when compared to normal distribution model.

# Conclusion

In this project, we saw how the VIX can be modeled using a POMP model and a GARCH model. The log likelihood of the GARCH model was greater. There were also issues with the convergence of some parameters in the POMP model. These factors lead us to say that the GARCH model is better at modeling the volatilty of the VIX index. However, this does not mean that the POMP model is useless. Some of the parameters in the POMP model have better interpretability due to its underlying leverage model. Meanwhile, the parameters of the GARCH model are less interpretable, so while we may model the index better, we don't learn too much about the nature of the market. 

This work gives evidence that the volatility of the volatility of the market can be modeled using models meant for the volatility of markets themselves. Further work could involve running larger POMP models requiring greater computation, as well as looking other volatility indexes that may exist for other markets.

# References

1. http://www.cboe.com/vix
2. Edward Ionides, “14. Case study: POMP modeling to investigate financial volatility”, https://ionides.github.io/531w18/14/notes14.html#arch-and-garch-models.
3. Bretó, C. (2014). On idiosyncratic stochasticity of financial leverage
efects, Statistics & Probability Letters 91: 20-26. URL: http://www.sciencedirect.com/science/article/pii/S0167715214001345
4. Brian Thelen, "Lecture 9: Introduction to ARCH/GARCH," University of Michigan STATS 509 Notes, Winter 2019.
5. Brian Thelen, "Lecture 10: Advanced Topics in GARCH Time Series Analysis," University of Michigan STATS 509 Notes, Winter 2019.

I got ideas for this project from the Winter 2018 final project titled "Investigation on Financial Volatility of NASDAQ" (no author listed). Code for this project was adapted from code from the course notes from throughout the semester.