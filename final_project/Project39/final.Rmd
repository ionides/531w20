---
title: "Researching on stochastic volatility of Tesla stock price in 2019"
output:
  html_document:
    number_sections: yes
    theme: flatly
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: no
      smooth_scroll: yes
---


# Abstract

In 2019, due to the intensification of the China-United States trade war, the global economy has been severely influenced in all aspects. The U.S. economy failed to achieve the 3% growth target set by the Trump administration for the second consecutive year. The annual growth rate in 2019 was the lowest in past three years, as corporate investment declined further under the demaging trade tensions. Tesla, as the largest electric vehicle and solar panel company in the United States, is worthwhile to focus on. Especially, in the first half of 2019, Tesla was still struggling with the decline of the market, the stock price fell to 190 per share, however, in the second half, stock price frequently hit the record highs, rising above 500 per share. After 2020, Tesla has become the first listed automobile company in the United States with a market value of over 100 billion dollars. In this paper, we will concentrate on the stochastic volitiy of Tesla's adjusted closing price by operating the GARCH model and the POMP model.


# Data description

Adjusted closing price amends a stock's closing price to accurately reflect that stock's value after accounting for any corporate actions. It is considered to be the true price of that stock and is often used when examining historical returns or performing a detailed analysis of historical returns. Our data selects Tesla's adjusted closing price in the whole 2019 for further analysis.


Our data source: Tesla stock data from 2010 to 2020 (https://www.kaggle.com/timoboz/tesla-stock-data-from-2010-to-2020)

+ `DATE`: Date from 2010 to 2020

+ `Open`: Opening price

+ `High`: Highest price that day

+ `Low`: Lowest price that day

+ `Close`: Closing Price

+ `Adj Close`: Adjusted closing price, taking splits etc into account

+ `Volume` : Trading volume


# Data preprocessing and visualization


```{r message=FALSE,warning=FALSE}
# Read data and Preprocessing

tsla <- read.csv("TSLA.csv",header = TRUE)
tsla <- tsla[c(2143:2394),]

tsla <- tsla[which(tsla$Adj.Close != "NA"),]

# Data visualization

tsla_log <- diff(log(tsla$Adj.Close))
tsla_dm <- tsla_log - mean(tsla_log)


y1 = range(tsla$Adj.Close, na.rm=TRUE)
y2 = range(tsla_dm,na.rm = TRUE)
plot(tsla$Adj.Close,type = "l",main = "Adjusted Close Price",ylim = y1,xlab = "Date",ylab = "Adj.Close")
abline(h=mean(tsla$Adj.Close),col = "blue")
plot(tsla_dm,type = "l",main = "Demeaned log Adjusted Close Price",ylim = y2,xlab = "Date",ylab = "Adj.Close")
abline(h=mean(tsla_dm),col = "blue")
```


As showed above, we obtained two plots for the original adjusted closing price and the demeaned adjusted closing price, respectively. From the plot of the original adjusted closing price, we could observe that the turbulent trend throughout the 2019, to be more specific, the overall decline in the first half, the overall rise in the second half. From the plot of the demeaned adjusted closing price, we could observe that it's a random pertubation around 0. It's worthy to be mentioned that the variance of the process is quite different during the different periods in 2019.


# POMP model

## Model description

Following the model representation of Breto, we propose a model,

$Y_n$ = exp{$H_n$/2}$\epsilon_n$

$H_n$ = $\mu_h$(1 - $\phi$) + $\phi$$H_{n-1}$ + $\beta_{n-1}$$R_n$exp{-$H_{n-1}$/2} + $\omega_n$

$G_n$ = $G_{n-1}$ + $\nu_n$

where $\beta_n$ = $Y_n$$\sigma_{\eta}$$\sqrt{1 - \phi^2}$,{$\epsilon_n$} is an iid N(0,1) sequence, {$\nu_n$} is an iid N(0,$\sigma_{\eta}^2$) sequence, and {$\omega_n$} is an iid N(0,$\sigma_{\omega}^2$) sequcence.

Here, $H_n$ is the log volatility.

We use the state variable $X_n$ = ($G_n$, $H_n$, $Y_n$) and model the measurement process as a perfect observation of the $Y_n$ component of the state space.


## Building a POMP model


Firstly, we construct the POMP model as following.



```{r message=FALSE,warning=FALSE}
# Building a POMP model

require(pomp)
tsla_statenames <- c("H","G","Y_state")
tsla_rp_names <- c("sigma_nu","mu_h","phi","sigma_eta")
tsla_ivp_names <- c("G_0","H_0")
tsla_paramnames <- c(tsla_rp_names,tsla_ivp_names)

rproc1 <- "
  double beta,omega,nu;
  omega = rnorm(0,sigma_eta * sqrt( 1- phi*phi ) *
    sqrt(1-tanh(G)*tanh(G)));
  nu = rnorm(0, sigma_nu);
  G += nu;
  beta = Y_state * sigma_eta * sqrt( 1- phi*phi );
  H = mu_h*(1 - phi) + phi*H + beta * tanh( G )
    * exp(-H/2) + omega;
"
rproc2.sim <- "
  Y_state = rnorm( 0,exp(H/2) );
"
rproc2.filt <- "
  Y_state = covaryt;
"
tsla_rproc.sim <- paste(rproc1,rproc2.sim)
tsla_rproc.filt <- paste(rproc1,rproc2.filt)

tsla_rinit <- "
  G = G_0;
H = H_0;
  Y_state = rnorm( 0,exp(H/2) );
"

tsla_rmeasure <- "
   y=Y_state;
"
tsla_dmeasure <- "
   lik=dnorm(y,0,exp(H/2),give_log);
"

# Parameter transformations

tsla_partrans <- parameter_trans(
  log=c("sigma_eta","sigma_nu"),
  logit="phi"
)
```


## Filtering on simulated data


Secondly, we filter on simulated data to check whether the basic particle is working or not. In the following, we set three different run levels and finally obtain loglikelihood is -257.20 with standard error 0.93, which helps us to know more about the estimation range of the related parameters.


```{r message=FALSE,warning=FALSE}
tsla.filt <- pomp(data=data.frame(
  y=tsla_dm,time=1:length(tsla_dm)),
  statenames=tsla_statenames,
  paramnames=tsla_paramnames,
  times="time",
  t0=0,
  covar=covariate_table(
    time=0:length(tsla_dm),
    covaryt=c(0,tsla_dm),
    times="time"),
  rmeasure=Csnippet(tsla_rmeasure),
  dmeasure=Csnippet(tsla_dmeasure),
  rprocess=discrete_time(step.fun=Csnippet(tsla_rproc.filt),
                         delta.t=1),
  rinit=Csnippet(tsla_rinit),
  partrans=tsla_partrans
)

params_test <- c(
  sigma_nu = exp(-4.5),
  mu_h = -0.25,
  phi = expit(4),
  sigma_eta = exp(-0.07),
  G_0 = 0,
  H_0=0
)
sim1.sim <- pomp(tsla.filt,
                 statenames=tsla_statenames,
                 paramnames=tsla_paramnames,
                 rprocess=discrete_time(
                   step.fun=Csnippet(tsla_rproc.sim),delta.t=1)
)
sim1.sim <- simulate(sim1.sim,seed=1,params=params_test)

sim1.filt <- pomp(sim1.sim,
                  covar=covariate_table(
                    time=c(timezero(sim1.sim),time(sim1.sim)),
                    covaryt=c(obs(sim1.sim),NA),
                    times="time"),
                  statenames=tsla_statenames,
                  paramnames=tsla_paramnames,
                  rprocess=discrete_time(
                    step.fun=Csnippet(tsla_rproc.filt),delta.t=1)
)

run_level <- 1
tsla_Np <- switch(run_level, 100, 1e3, 2e3)
tsla_Nmif <- switch(run_level,  10, 100, 200)
tsla_eval <- switch(run_level,   4,  10,  20)
tsla_local <- switch(run_level,  10,  20,  20)
tsla_global <- switch(run_level,  10,  20, 40)

library(doParallel)
registerDoParallel()
library(doRNG)
registerDoRNG(34118892)

stew(file=sprintf("pf1-%d.rda",run_level),{ t.pf1 <- system.time(
  pf1 <- foreach(i=1:tsla_eval,
                 .packages='pomp') %dopar% pfilter(sim1.filt,Np=tsla_Np))
},seed=493536993,kind="L'Ecuyer")
(L.pf1 <- logmeanexp(sapply(pf1,logLik),se=TRUE))
```




## Fitting the stochastic leverage model to Tesla data


Thirdly, we operate a local search on logLikelihood surface of stochastic volatility model to Tesla data.


```{r message=FALSE,warning=FALSE}
run_level <- 2
tsla_Np <- switch(run_level, 100, 1e3, 1e4)
tsla_Nmif <- switch(run_level,  10, 100, 200)
tsla_eval <- switch(run_level,   4,  10,  20)
tsla_local <- switch(run_level,  10,  20,  20)
tsla_global <- switch(run_level,  10,  20, 40)

tsla_rw.sd_rp <- 0.02
tsla_rw.sd_ivp <- 0.1
tsla_cooling.fraction.50 <- 0.5
tsla_rw.sd <- rw.sd(
  sigma_nu  = tsla_rw.sd_rp,
  mu_h      = tsla_rw.sd_rp,
  phi       = tsla_rw.sd_rp,
  sigma_eta = tsla_rw.sd_rp,
  G_0       = ivp(tsla_rw.sd_ivp),
  H_0       = ivp(tsla_rw.sd_ivp)
)

stew(file=sprintf("mif1-%d.rda",run_level),{ t.if1 <- system.time({
  if1 <- foreach(i=1:tsla_local,
                 .packages='pomp', .combine=c) %dopar% mif2(tsla.filt,
                                                            params=params_test,
                                                            Np=tsla_Np,
                                                            Nmif=tsla_Nmif,
                                                            cooling.fraction.50=tsla_cooling.fraction.50,
                                                            rw.sd = tsla_rw.sd)
  L.if1 <- foreach(i=1:tsla_local,
                   .packages='pomp', .combine=rbind) %dopar% logmeanexp(
                     replicate(tsla_eval, logLik(pfilter(tsla.filt,
                                                             params=coef(if1[[i]]),Np=tsla_Np))), se=TRUE) })
},seed=318817883,kind="L'Ecuyer")
r.if1 <- data.frame(logLik=L.if1[,1],logLik_se=L.if1[,2],
                    t(sapply(if1,coef)))
if (run_level>1) write.table(r.if1,file="TSLA.csv",
                             append=TRUE,col.names=FALSE,row.names=FALSE)
summary(r.if1$logLik,digits=5)
r.if1[which.max(r.if1$logLik),]
plot(if1)
pairs(~logLik+sigma_nu+mu_h+phi+sigma_eta,
      data=subset(r.if1,logLik>max(logLik)-500))
```


From the results above, 

1. We could see that the largest logLikelihood is 542.0 for this POMP model.

2. The likelihood converges quickly, with an estimated mean of 513.7, and once it converged, it becomes quitely stable. $H_0$ are not shrinkage, however, since $H_0$ is just the starting point of the POMP model, there are naturally large fluctuations and we don't need to pay more attention on this phenomenon. Almost all parameters are not convergent, however, if you observe carefully, we could find that they all stay in very small range of intervals. Considering about the small sample size, we could conclude that this result is basically satisfied.



## Likelihood maximization using randomized starting values


Finally, we search on likelihood starting randomly throughout a large box. We could obtain the similar results and conclusions as what in the local search method above. We think that maybe we could refine the parameters box and then run the alogrithms to optimize the results.


```{r message=FALSE,warning=FALSE}
tsla_box <- rbind(
  sigma_nu=c(0.005,0.05),
  mu_h    =c(-1,0),
  phi = c(0.95,0.99),
  sigma_eta = c(0.5,1),
  G_0 = c(-2,2),
  H_0 = c(-1,1)
)

stew(file=sprintf("box_eval-%d.rda",run_level),{ t.box <- system.time({
  if.box <- foreach(i=1:tsla_global,
                    .packages='pomp',.combine=c) %dopar% mif2(if1[[1]],
                                                              params=apply(tsla_box,1,function(x)runif(1,x)))
  L.box <- foreach(i=1:tsla_global,
                   .packages='pomp',.combine=rbind) %dopar% { logmeanexp(replicate(tsla_eval, logLik(pfilter(
                     tsla.filt,params=coef(if.box[[i]]),Np=tsla_Np))),
                     se=TRUE)
                   } })
},seed=290860873,kind="L'Ecuyer")
r.box <- data.frame(logLik=L.box[,1],logLik_se=L.box[,2],
                    t(sapply(if.box,coef)))
if(run_level>1) write.table(r.box,file="TSLA.csv",
                            append=TRUE,col.names=FALSE,row.names=FALSE)
summary(r.box$logLik,digits=5)
r.box[which.max(r.box$logLik),]

pairs(~logLik+log(sigma_nu)+mu_h+phi+sigma_eta+H_0,
      data=subset(r.box,logLik>max(logLik)-500))

plot(if.box)
```


# GRACH model


## Model Description

The generalized ARCH model, known as GARCH(p,q), has the form $Y_n$ = $\epsilon_n$$\sqrt{V_n}$, 

where $V_n$ = $\alpha_0$ + $\sum_{j=1}^{p}\alpha_j$$Y_{n-j}^2$ + $\sum_{k=1}^{q}\beta_k$$V_{n-k}$ and $\epsilon_{1:N}$ is white noise.


## Building the GARCH(1,1) model


```{r message=FALSE,warning=FALSE}
library(tseries)
require(fGarch)
garch11 <- garchFit(~garch(1,1), data = tsla_dm, cond.dist = c("norm"), include.mean = FALSE, algorithm = c("nlminb"), hessian = c("ropt"))
summary(garch11)
```


## Check the 95% interval for the GARCH(1,1) model


```{r message=FALSE,warning=FALSE}
t11 = garch11@sigma.t
plot(tsla_dm, ylim = c(-0.2,0.2), ylab = 'Demeaned tsla Adjusted Close Price', xlab = 'Date', type = 'l', main = 'Garch(1,1)', lwd = 1)
lines(-2*t11, lty=2, col='grey', lwd = 1.5)
lines(2*t11, lty=2, col='grey', lwd = 1.5)
legend('topright', c('Demeaned tsla Adjusted Close Price','95% interval'), col = c('black','grey'), lty = c(1,2), lwd = c(1,1.5))
```

## QQ-plot 

```{r message=FALSE,warning=FALSE}
qqnorm(tsla_dm)
qqline(tsla_dm)
```



From the results above, we could obtain that the logLikelihood for GARCH(1,1) is 517.1, which is smaller than it in POMP model.



# Conclusion

1. Comparing with the maximum logLikelihood between the POMP model and GARCH(1,1) model, the values of the logLikelihood are close to each other. If we base on the AIC criteria, we could elect the both of them since they have similarly good performance to predict the stochastic volatility of Tesla stock adjusted closing price.

2. The Tesla stock adjusted closing price in 2019 violates the assumption of the GARCH model that the residuals should have normality. This point is worthwhile to further focus on, maybe we could do something, like expanding sample size to optimize the GARCH model.

3. From the diagnostics of the POMP model of stochastic volatility, almost all the parameters are not convergent, however, they flucturate in relatively small ranges. We guess that we could refine the parameters box and expand sample size to solve this problem. 

4. To be honest, the GRACH model and stochastic volatility model for the Tesla stock adjusted closing price both show unsatisfacory at a certain degree. We have to further research on optimizing the existed model or putting forward the new approach.


# Reference

1. Stochastic Volatility of the SPX500 Index ("https://ionides.github.io/531w16/final_project/Project14/Final.html")

2. Financial Volatility of Google Stock ("https://ionides.github.io/531w18/final_project/1/final.html")

3. Investigation on Financial Volatility of NASDAQ ("https://ionides.github.io/531w18/final_project/2/final.html")

4. Stochastic Volatility of Nasdaq index ("https://ionides.github.io/531w18/final_project/16/final.html")

5. Analysis of SP500 Volatility ("https://ionides.github.io/531w18/final_project/19/final.html")

6. Time Series Analysis of Nintendo stock price ("https://ionides.github.io/531w18/final_project/27/final.html")

7. Analyzing the Volatility of Bitcoin Market Price ("https://ionides.github.io/531w18/final_project/35/final.html")

8. Case study: POMP modeling to investigate financial volatility ("https://ionides.github.io/531w20/14/notes14.pdf")

9. Can log likelihood function be positive- Cross Validated ("https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=2ahUKEwixwL_hp47pAhVXYs0KHbFJAcMQFjACegQIDBAG&url=https%3A%2F%2Fstats.stackexchange.com%2Fquestions%2F319859%2Fcan-log-likelihood-funcion-be-positive&usg=AOvVaw2a1dWxy4kc55mNfj0CAyuE")

10. 2019 USA Economy ("https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwjJ9buvqY7pAhXQKs0KHZRABQsQFjAAegQIBBAB&url=https%3A%2F%2Fwww.reuters.com%2Farticle%2Fwrapup-us-2019-economy-trump-target-0131-idCNKBS1ZU092&usg=AOvVaw2-j-JOqzvQUlTRh-x4qCV4")

11. Tesla 2019 Financial Report ("https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwiM5ObgqY7pAhWaK80KHblbDb0QFjAAegQIARAB&url=https%3A%2F%2Fauto.sina.cn%2Fnews%2Fhy%2F2020-02-06%2Fdetail-iimxxste9398111.d.html&usg=AOvVaw3ZDGExRI4Jnyi9UtXrMQUE")
